## PKU-Alignment

Large language models (LLM) have immense potential in the field of general intelligence but come with significant risks. As a research team at Peking University, we are actively focusing on alignment techniques for large language models, such as safe-alignment to enhance the model's safety and reduce its toxicity.

Welcome to follow our AI Safety project:
- [safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/safe-rlhf)](https://github.com/PKU-Alignment/safe-rlhf/stargazers)
- [omnisafe](https://github.com/PKU-Alignment/omnisafe) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/omnisafe)](https://github.com/PKU-Alignment/omnisafe/stargazers)
- [safepo](https://github.com/PKU-Alignment/Safe-Policy-Optimization) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/Safe-Policy-Optimization)](https://github.com/PKU-Alignment/Safe-Policy-Optimization/stargazers)
- [safety-gymnasium](https://github.com/PKU-Alignment/safety-gymnasium) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/safety-gymnasium)](https://github.com/PKU-Alignment/safety-gymnasium/stargazers)
