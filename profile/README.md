## PKU-Alignment

Large language models (LLM) have immense potential in the field of general intelligence but come with significant risks. As a research team at Peking University, we actively focus on alignment techniques for large language models, such as safety alignment, to enhance the model's safety and reduce toxicity.

Welcome to follow our AI Safety project:
- [align-anything](https://github.com/PKU-Alignment/align-anything) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/align-anything)](https://github.com/PKU-Alignment/align-anything/stargazers)
- [safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/safe-rlhf)](https://github.com/PKU-Alignment/safe-rlhf/stargazers)
- [omnisafe](https://github.com/PKU-Alignment/omnisafe) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/omnisafe)](https://github.com/PKU-Alignment/omnisafe/stargazers)
- [safepo](https://github.com/PKU-Alignment/Safe-Policy-Optimization) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/Safe-Policy-Optimization)](https://github.com/PKU-Alignment/Safe-Policy-Optimization/stargazers)
- [safety-gymnasium](https://github.com/PKU-Alignment/safety-gymnasium) [![GitHub stars](https://img.shields.io/github/stars/PKU-Alignment/safety-gymnasium)](https://github.com/PKU-Alignment/safety-gymnasium/stargazers)
